---
title: "Mini Snorkel Feather HSI Analysis Exploration"
author: "Maddee Rubenson"
date: "`r Sys.Date()`"
output: github_document
editor_options: 
  markdown: 
    wrap: 72
---

## Objective 

The following markdown reviews and compares literature approaches to calculating HSIs and is meant to be an exploratory analysis.


```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)

knitr::opts_chunk$set(echo = TRUE)

theme_set(theme_minimal())

ihs <- trans_new("ihs", 
                 transform = function(x) asinh(x), 
                 inverse = function(y) sinh(y), 
                 breaks = function(i) scales::breaks_log(n=5, base=10)(pmax(i,0.01)), 
                 #minor_breaks = scales::minor_breaks_n(n = 0),
                 domain=c(0, Inf),
                 format = scales::label_comma())

```

## Literature Notes

[**2004a**](https://netorg629193.sharepoint.com/:b:/s/VA-FeatherRiver/EY9qLwY15ypFn3W42E02takBH745JkefSPAbS4y9VFzuBQ?e=n2QRXK)
Data Analysis

-   Stepwise binary logistic regression analysis was used to assess
    factors influencing the occurrence of steelhead and Chinook salmon.
    Fine scale survey results were analyzed at both the mesohabitat (100
    m2) and microhabitat (1 m2) scale.

-   Mesohabitat analysis was performed by treating the entire 25 m reach
    as a sample

-   Reach habitat variables where steelhead or salmon were present
    (logistic response variable) were compared to reaches where fish
    were absent (logistic reference variable)

-   Microhabitat was done similarly except that individual one square
    meter cells were considered rather than entire reaches

-   Reaches lacking salmon or steelhead were not included in
    microhabitat analysis

[**2005**](https://netorg629193.sharepoint.com/:b:/s/VA-FeatherRiver/ES8H3f7ZUO5Aj7OK8hfW03UB721tCgZd_OEb8P9cbJLiMA?e=L95bf8):
HSC development- chinook salmon rearing

-   HSC were created for fry (\<50 mm) and juvenile Chinook salmon depth
    and mean column velocity data using a three-point running mean to
    smooth frequency distributions of the fry habitat use data and using
    NPTL [what does this stand for?] for the juvenile habitat use data.

-   all substrate suitability given value of 1 after finding that
    substrate was not driving factor for microhabitat selection of fry
    and juveniles

-   cover modified to: with and without. Suitability of without cover
    was calculated as the percentage of fish observed without cover to
    the total sample size

-   suitability of cover present was assigned a value of 1.0 and 0.30 or
    0.22 for cover absent for Chinook salon fry and juveniles [I do not
    get this... ]

-   Focus on suitability of instream cover. When a cover variable did
    not exist, preference for low velocity and shallow depth in a large
    river indicate suitable habitat along stream margins or out in the
    main channel when the river is nearly dry and the preferred
    conditions are prevalent.

-   intermediate scale data separated into four cover types: no cover,
    object only cover overhead only cover, both object and overhead
    cover

[**Gard
2023**](https://netorg629193.sharepoint.com/:b:/s/VA-FeatherRiver/EelfImRfhzxKjLQdrstbIPgBqbRV5S0Wke5GkSh06_CUrQ?e=6PdWKh) -
HSC comparison

-   Presence/absence HSC are developed using a polynomial logistic
    regression that uses both the occupied and unoccupied data; the
    results are then rescaled that the highest value is 1 to calculate
    the HSI

-   For depth and velocity HSC, the criteria were developed directly
    from use observations using a range of curve fitting and smoothing
    techniques

-   Use/availability criteria are developed by dividing use
    observations, generally binned, by availability data from transects

```{r}
# read in mini snorkel data
mini_snorkel_model_ready <- read_csv(here::here('data-raw', 'microhabitat_with_fish_observations.csv')) |> 
  mutate(count = ifelse(is.na(count), 0, count),
         fish_presence = as.factor(ifelse(count < 1, "0", "1"))) |> 
  glimpse()

```

<!-- ### Explore Variables -->

<!-- Specifically looking for collinearity in the variables -->

```{r eval=FALSE, include=FALSE}
chn_mini_snorkel_substrate <- mini_snorkel_model_ready |> 
  filter(species == "Chinook salmon" | is.na(species)) |> 
  select(fish_presence, percent_fine_substrate:percent_boulder_substrate) 

cor_matrix <- cor(chn_mini_snorkel_substrate |> select(-fish_presence))

# print(cor_matrix)

heatmap(cor_matrix, 
        col = colorRampPalette(c("blue", "white", "red"))(20),
        symm = TRUE,
        margins = c(10, 10))
```

### Logistic Regression Using Cover, Substrate, Velocity, and Depth

**Predictors**

-   Depth

-   Velocity

-   Substrate (fine through boulder) normalized by prevalence

-   Woody Debris (`percent_small_woody_cover_inchannel` +
    `percent_large_woody_cover_inchannel`)

-   Overhead Cover (`percent_cover_more_than_half_meter_overhead` +
    `percent_cover_half_meter_overhead`)

-   Submerged Aquatic Vegetation

-   Undercut Bank

-   Surface Turbidity

#### Normalize Substrate by Prevalence

This table provides a weighting for each substrate type based on the
overall presence (\>20%) of each substrate type. Use this to normalize
the substrate columns.

```{r}
substrate_percent <- mini_snorkel_model_ready |> 
  group_by(micro_hab_data_tbl_id) |> 
  select(contains('substrate')) |> 
  distinct() |> 
  pivot_longer(cols = c(percent_fine_substrate:percent_boulder_substrate), names_to = "substrate_type", values_to = "percent") |> 
  mutate(substrate_presence_absence = ifelse(percent < 19, 0, 1)) |>  # 20% threshold
  group_by(substrate_type) |> 
  summarise(total_presence = sum(substrate_presence_absence)) |> 
  ungroup() |> 
  mutate(perc_total = total_presence/sum(total_presence))

knitr::kable(substrate_percent |> mutate(perc_total = perc_total*100), digits = 2)

```

#### Apply table to substrate columns to normalize

```{r}
mini_snorkel_grouped <- mini_snorkel_model_ready |> 
  rowwise() |> 
  mutate(fine_substrate = percent_fine_substrate * (1-substrate_percent[substrate_percent$substrate_type == "percent_fine_substrate", 'perc_total']$perc_total),
         sand_substrate = percent_sand_substrate * (1-substrate_percent[substrate_percent$substrate_type == "percent_sand_substrate", 'perc_total']$perc_total),
         small_gravel = percent_small_gravel_substrate * (1-substrate_percent[substrate_percent$substrate_type == "percent_small_gravel_substrate", 'perc_total']$perc_total),
         large_gravel = percent_large_gravel_substrate * (1-substrate_percent[substrate_percent$substrate_type == "percent_large_gravel_substrate", 'perc_total']$perc_total),
         cobble_substrate = percent_cobble_substrate * (1-substrate_percent[substrate_percent$substrate_type == "percent_cobble_substrate", 'perc_total']$perc_total),
         boulder_substrate = percent_boulder_substrate * (1-substrate_percent[substrate_percent$substrate_type == "percent_boulder_substrate", 'perc_total']$perc_total)) |> 
  select(-c(percent_fine_substrate:percent_boulder_substrate)) |> 
  mutate(woody_debris = sum(percent_large_woody_cover_inchannel, percent_small_woody_cover_inchannel),
         overhead_cover = sum(percent_cover_half_meter_overhead, percent_cover_more_than_half_meter_overhead)
         ) |> 
  select(-c(percent_small_woody_cover_inchannel, percent_large_woody_cover_inchannel, percent_cover_more_than_half_meter_overhead, percent_cover_half_meter_overhead)) |> 
  #filter(species == "Chinook salmon" | is.na(species)) |> 
  select(-count, -channel_geomorphic_unit, -micro_hab_data_tbl_id, -location_table_id, -fish_data_id,
         -focal_velocity, -dist_to_bottom, -fl_mm, -species,  -transect_code, -date) |> 
  select(-(contains("no_cover"))) |> 
  distinct() |> 
  na.omit() |> 
  glimpse()
```

#### Explore newly constructed variables

```{r echo=FALSE}
ggplot(mini_snorkel_grouped, aes(fine_substrate)) +
  geom_density()

ggplot(mini_snorkel_grouped, aes(sand_substrate)) +
  geom_density()

ggplot(mini_snorkel_grouped, aes(small_gravel)) +
  geom_density()

ggplot(mini_snorkel_grouped, aes(large_gravel)) +
  geom_density()

ggplot(mini_snorkel_grouped, aes(boulder_substrate)) +
  geom_density()

ggplot(mini_snorkel_grouped, aes(woody_debris)) +
  geom_density()

ggplot(mini_snorkel_grouped, aes(overhead_cover)) +
  geom_density()
```

```{r eval=FALSE, include=FALSE}
library(AppliedPredictiveModeling)
transparentTheme(trans = .4)
library(caret)

mini_snorkel_by_species <- mini_snorkel_grouped |> Matrix::as.matrix() 

featurePlot(x = as.numeric(mini_snorkel_by_species[,1]), 
            y = mini_snorkel_by_species[,6], 
            plot = "ellipse",
            ## Add a key at the top
            auto.key = list(columns = 3))

```

```{r eval=FALSE, include=FALSE}
descrCor <-  cor(mini_snorkel_grouped)
highCorr <- sum(abs(descrCor[upper.tri(descrCor)]) > .999)

comboInfo <- findLinearCombos(mini_snorkel_grouped |> as_tibble() |>  select(-fish_presence))

```

```{r include=FALSE}
library(caret)

test <- caret::preProcess(mini_snorkel_grouped, method = c("center", "scale", "YeoJohnson"))
test

# Yeo Johnson:
test[["method"]][["YeoJohnson"]]

test[["method"]][["center"]]

test[["method"]][["scale"]]


test_transformed <- predict(test, mini_snorkel_grouped) |> na.omit()

names_change <- names(test_transformed)
names(test_transformed) <- paste0(names(test_transformed), "_transformed")
test_transformed <- test_transformed |> rename(fish_presence = fish_presence_transformed)


ggplot() +
  geom_histogram(data = test_transformed, aes(depth_transformed)) 

ggplot() +
geom_histogram(data = mini_snorkel_grouped, aes(log(depth)), alpha = 0.5)

# ggplot() +
#   geom_histogram(data = test_transformed, aes(overhead_cover_transformed)) 
# 
# ggplot() +
# geom_histogram(data = mini_snorkel_grouped, aes(overhead_cover), alpha = 0.5)



```

```{r include=FALSE}
options(scipen=999)

# Split the data into training and testing sets
data_split <- initial_split(mini_snorkel_grouped, prop = 0.8, strata = "fish_presence")
data_train <- training(data_split)
data_test <- testing(data_split)

recipe <- recipe(data = mini_snorkel_grouped, formula = fish_presence ~.) |> 
  step_mutate_at(all_numeric_predictors(), fn = asinh) |> 
  step_naomit(all_predictors()) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_numeric_predictors())

log_reg <- logistic_reg() |> 
  set_engine("glm") 

log_reg_model <- workflow() |>
  add_recipe(recipe) |>
  add_model(log_reg) |> 
  fit(data = mini_snorkel_grouped)

log_reg_model |> glance()
log_reg_model |> tidy() |> filter(p.value < 0.1)

sig_predictors <- log_reg_model |> tidy() |> filter(p.value < 0.1) |> filter(term != "(Intercept)") |>  pull(term)

print(sig_predictors)

predictions <- predict(log_reg_model, mini_snorkel_grouped, type = "prob") |> 
  bind_cols(mini_snorkel_grouped) |> glimpse()

map(sig_predictors, function(predictor) {
  ggplot(predictions, aes_string(x = predictor, y = ".pred_1")) +
    geom_smooth() +
    labs(title = paste("Predicted Probability vs.", predictor))
})

```

#### Create Logistic Regression

```{r eval=FALSE, include=FALSE}
model <- glm(fish_presence ~., data = test_transformed, family = binomial)

probabilities <- predict(model, type = "response")
predicted_classes <- ifelse(probabilities > 0.5, "1", "0") 

# influential values
plot(model, which = 4, id.n = 3)

# Extract model results
model_data <- augment(model) %>% 
  mutate(index = 1:n()) 

model_data |> top_n(3, .cooksd) # top three largest influential values (as seen in plot)

#standard resuiduals: a measure of the strength of the difference between observed and expected values. 
ggplot(model_data, aes(index, .std.resid)) + 
  geom_point(aes(color = fish_presence), alpha = .5) 

# Filter potential influential data points with abs(.std.res) > 3:
chn_mini_snorkel_filtered <- model_data %>% 
  filter(abs(.std.resid) < 3)

#the linearly dependent variables. These variables are too linear and need to be removed from 
# the model formula: 
ld_vars <- attributes(alias(model)$Complete)$dimnames[[1]]
print(ld_vars)

chn_mini_snorkel_filtered_without_linear_cols <- chn_mini_snorkel_filtered |> 
  select(-ld_vars, -(.fitted:index)) 

model_2 <- glm(fish_presence ~., data = chn_mini_snorkel_filtered_without_linear_cols, family = binomial)

# test for multicollinarity:
# As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.
car::vif(model_2) 

summary(model_2)

model_3 <- stats::step(model, test = "LRT")
summary(model_3)


probabilities_model_3 <- predict(model_3, type = "response")


```

```{r eval=FALSE, include=FALSE}
probabilities_model_3 <- predict(model_3, newdata = mini_snorkel_grouped, type = "response")
hist(probabilities_model_3)

to_plot <- test_transformed |> 
  bind_cols(data.frame("probabliliites" = probabilities_model_3)) |> 
  bind_cols(mini_snorkel_grouped |> select(-fish_presence)) |> glimpse()

ggplot(to_plot, aes(x = surface_turbidity, y = probabilities)) +
  geom_smooth()

ggplot(to_plot, aes(x = overhead_cover, y = probabilities)) +
  geom_smooth()

ggplot(to_plot, aes(x = woody_debris, y = probabilities)) +
  geom_smooth()

ggplot(to_plot, aes(x = small_gravel, y = probabilities)) +
 geom_point() + 
   geom_smooth()

ggplot(to_plot, aes(x = percent_undercut_bank, y = probabilities)) +
  geom_point()

```

```{r eval=FALSE, include=FALSE}
library(patchwork)
library(ggeffects)

plts = lapply(names(coefficients(model_3))[-1],function(i){
       return(plot(ggpredict(model_3,i)))
       })

plts

```

## Logistic Regression for Substrate and Cover

Build a logistic regression of fish presence and absence to identify
important cover and substrate variables.

**Findings**

-   Substrate: most important predictor is small gravel
-   Cover: all predictors (aside from the two NAs and
    `percent_cover_half_meter_overhead`) were significant.
    `percent_no_cover_overhead` was most significant.

**Questions**

-   Substrate: boulder came out as NA in model. Should explore why this
    occurred.
-   Cover: `percent_undercut_bank` and
    `percent_cover_more_than_half_meter_overhead` came back as NA. Need
    to explore.

#### Substrate Logistic Regression

```{r}
set.seed(06221988)

# start with Chinook and substrate 
chn_mini_snorkel <- mini_snorkel_model_ready |> 
  filter(species == "Chinook salmon" | is.na(species)) |> 
  select(fish_presence, percent_fine_substrate:percent_boulder_substrate) 

# Define a recipe
rec <- recipe(fish_presence ~  percent_fine_substrate + percent_sand_substrate + percent_small_gravel_substrate + percent_large_gravel_substrate + percent_boulder_substrate + percent_cobble_substrate, data = chn_mini_snorkel)  

# Split the data into training and testing sets
data_split <- initial_split(chn_mini_snorkel, prop = 0.8, strata = "fish_presence")
data_train <- training(data_split)
data_test <- testing(data_split)

# Create a logistic regression model
log_reg <- logistic_reg() |> 
  set_engine("glm") |> 
  set_mode("classification") |> translate()

# Create a workflow
wf <- workflow()  |> 
  add_recipe(rec) |> 
  add_model(log_reg)

# Train the model
wf_fit <- wf |> 
  fit(data_train)

tidy(wf_fit) # why is boulder NA? small gravel only significant predictor
glance(wf_fit)

# Make predictions
predictions <- predict(wf_fit, data_test) |>
  bind_cols(data_test)
```

#### Cover Logistic Regression

```{r}
# start with Chinook and substrate 
chn_mini_snorkel <- mini_snorkel_model_ready |> 
  filter(species == "Chinook salmon" | is.na(species)) |> 
  select(fish_presence, percent_no_cover_inchannel:percent_cover_more_than_half_meter_overhead) |> 
  na.omit() 

# Define a recipe
rec <- recipe(fish_presence ~  percent_no_cover_inchannel + percent_small_woody_cover_inchannel + percent_large_woody_cover_inchannel + percent_submerged_aquatic_veg_inchannel + percent_undercut_bank + percent_no_cover_overhead + percent_cover_half_meter_overhead + percent_cover_more_than_half_meter_overhead, data = chn_mini_snorkel)  

# Split the data into training and testing sets
data_split <- initial_split(chn_mini_snorkel, prop = 0.8, strata = "fish_presence")
data_train <- training(data_split)
data_test <- testing(data_split)

# Create a logistic regression model
log_reg <- logistic_reg() |> 
  set_engine("glm") |> 
  set_mode("classification") |> translate()

# Create a workflow
wf <- workflow()  |> 
  add_recipe(rec) |> 
  add_model(log_reg)

# Train the model
wf_fit <- wf |> 
  fit(data_train)

tidy(wf_fit) # 

# Make predictions
# predictions <- predict(wf_fit, data_test) |>
#   bind_cols(data_test)
```

### Adapted Mark Gard 1998 HSC process to Cover and Substrate

Mark Gard's 1998 paper identified a method to developing HSC for
presence and absence of redds in different substrate classes. I will
follow the same methodology using fish presence/absence and total fish
count instead of redds and also apply to cover.

1.  Determine number of fish with each substrate-size class
2.  Calculate the proportion of fish with each substrate size class
3.  Calculate the HSC value for each substrate size class by dividing
    the proportion of fish in each substrate class by the proportion of
    fish with the most frequent substrate class

To convert from percent to discrete:

-   Cover decision: 20% or greater is considered present

-   Substrate decision: used same criteria as cover (for now)

**Discussion**

Substrate:

-   Sand has a stronger HSI when looking at number of fish vs.
    presence/absence

-   Presence/absence results are consistent with logistic regression

Cover:

-   More consistent results when looking at number of fish vs.
    presence/absence

```{r}
mark_gard_1998_hsi <- function(data, percent_presence = 20, cols = c()) {
  data_tidy <- data |> 
    select(count, fish_presence, cols) |> 
    pivot_longer(cols = cols, names_to = "type", values_to = "percent") |> 
    mutate(presence = ifelse(percent >= percent_presence, 1, 0)) |> 
    filter(presence == 1) |> 
    group_by(type) |>
    summarise(n_fish_presence = sum(as.numeric(fish_presence)),
              n_fish_total = sum(count)) |> 
    mutate(prop_fish_presence = n_fish_presence/sum(n_fish_presence),
           prop_fish_total = n_fish_total/sum(n_fish_total)) |> 
    ungroup()
  
     most_freq_total <- data_tidy |> 
      arrange(desc(prop_fish_total)) |> 
      slice(1) 
 
     most_freq_presence <- data_tidy |> 
      arrange(desc(prop_fish_presence)) |> 
      slice(1) 
  
  
  data_tidy_final <- data_tidy |> 
    mutate(hsc_presence = prop_fish_presence/most_freq_presence$prop_fish_presence,
           hsc_total = prop_fish_total/most_freq_total$prop_fish_total)
  
  return(data_tidy_final)

}
```

#### Substrate

```{r}
cols <- c('percent_fine_substrate', "percent_sand_substrate" , 'percent_small_gravel_substrate', 'percent_large_gravel_substrate', 'percent_cobble_substrate', 'percent_boulder_substrate')

hsi_substrate <- mark_gard_1998_hsi(data = mini_snorkel_model_ready, percent_presence = 20, cols = cols)

hsi_substrate |> 
  pivot_longer(cols = c(hsc_presence, hsc_total), names_to = "hsc", values_to = "values") |> 
  ggplot() + 
  geom_col(aes(y = values, x = type, fill = hsc), position = "dodge") +
  coord_flip() +
  ggtitle('Mark Gard 1998: Substrate HSI Comparison')

```

#### Cover

**Notes**

-   Removed no cover variables

-   Created two cover HSIs: in-channel and overhead

```{r}
cols_inchannel <- c('percent_small_woody_cover_inchannel', 'percent_large_woody_cover_inchannel', 
          'percent_submerged_aquatic_veg_inchannel', 'percent_undercut_bank') #'percent_no_cover_inchannel'

cols_overhead <- c('percent_cover_half_meter_overhead',
          'percent_cover_more_than_half_meter_overhead') #'percent_no_cover_overhead'

hsi_cover <- mark_gard_1998_hsi(data = mini_snorkel_model_ready, percent_presence = 20, cols = cols_inchannel)

hsi_cover |> 
  pivot_longer(cols = c(hsc_presence, hsc_total), names_to = "hsc", values_to = "values") |> 
  ggplot() + 
  geom_col(aes(y = values, x = type, fill = hsc), position = "dodge") +
  coord_flip() +
  ggtitle('Mark Gard 1998: In Channel Cover HSI Comparison')

hsi_cover_overhead <- mark_gard_1998_hsi(data = mini_snorkel_model_ready, percent_presence = 20, cols = cols_overhead)

hsi_cover_overhead |> 
  pivot_longer(cols = c(hsc_presence, hsc_total), names_to = "hsc", values_to = "values") |> 
  ggplot() + 
  geom_col(aes(y = values, x = type, fill = hsc), position = "dodge") +
  coord_flip() +
  ggtitle('Mark Gard 1998: Overhead Cover HSI Comparison')
```

<!-- ## Velocity and Depth -->

```{r eval=FALSE, include=FALSE}

ggplot(mini_snorkel_model_ready) + 
  geom_point(aes(x = count, y = velocity)) 

ggplot(mini_snorkel_model_ready) + 
  geom_boxplot(aes(x = fish_presence, y = velocity)) 

ggplot(mini_snorkel_model_ready) + 
  geom_point(aes(x = count, y = depth)) 

ggplot(mini_snorkel_model_ready) + 
  geom_boxplot(aes(x = fish_presence, y = depth)) 

```

### Adapted Mark Gard 2023 HSC approach for Velocity and Depth

Mark Gard (2023) listed three approaches to calculating the HSI for
depth and velocity. Each approach is attempted below.

#### Approach 1: Criteria from use observations using curve fitting

```{r}
lm_data <- mini_snorkel_model_ready |> 
  select(count, depth, velocity) |> 
  na.omit()

fit <- lm(count ~ poly(depth, 3) + poly(velocity, 3) , data = lm_data)
predicted_values <- predict(fit, type = "response")
lm_data$predicted_values <- predicted_values

tidy(fit)

lm_data %>%
  ggplot(aes(x = depth, y = count)) +
  geom_point() +
  geom_smooth(aes(x = depth, y = predicted_values), color = "blue") +
  labs(x = "Depth", y = "Fish Count") +
  theme_minimal()

lm_data %>%
  ggplot(aes(x = velocity, y = count)) +
  geom_point() +
  geom_smooth(aes(x = velocity, y = predicted_values), color = "blue") +
  labs(x = "Velocity", y = "Fish Count") +
  theme_minimal()
```

#### Approach 2: Use/Availability criteria from binned data

Use/availability criteria are developed by dividing use observations,
generally binned, by availability data from transects.

TODO: unsure how to proceed with this approach..

```{r}
approach_2 <- mini_snorkel_model_ready |> 
  select(count, fish_presence, depth, velocity) |> 
  na.omit()

```

#### Approach 3: Presence/absence HSC using logistic regression

```{r}

log_reg_data <- mini_snorkel_model_ready |> 
  select(fish_presence, depth, velocity) |> 
  na.omit()

# Fit polynomial logistic regression model
model <- glm(fish_presence ~ poly(depth, 2) + poly(velocity, 2), data = log_reg_data, family = binomial())

tidy(model)
summary(model)

stats::step(model, test = "LRT")

# depth, 2 only significant predictor 

model_2 <- glm(fish_presence ~ poly(depth, 2), data = log_reg_data, family = binomial())

summary(model_2)

# Calculate HSI values
predicted_vals <- predict.glm(model_2, type = "response")
model_2_prediction <- log_reg_data |> 
  mutate(predicted_vals = predicted_vals,
         binary_predicted_vals <- ifelse(predicted_vals <= 0.5, 0, 1))

ggplot(log_reg_data) + 
  geom_point(aes(x = depth, y = predicted_vals)) + 
  geom_smooth(aes(x = depth, y = predicted_vals), color = "blue") 

ggplot(log_reg_data) + 
  geom_point(aes(x = velocity, y = predicted_vals)) + 
  geom_smooth(aes(x = velocity, y = predicted_vals), color = "blue") 

```

## Alternative Mark Allen et al. Approach

[Seasonal microhabitat use by juvenile spring Chinook salmon in the
Yakima River Basin,
Washington](https://www.researchgate.net/publication/283579544_Seasonal_microhabitat_use_by_juvenile_spring_Chinook_salmon_in_the_Yakima_River_Basin_Washington?enrichId=rgreq-24ca3dc3af200b9ac3ffc17b07b2e506-XXX&enrichSource=Y292ZXJQYWdlOzI4MzU3OTU0NDtBUzoyOTM5MjI4ODM1NTUzMzNAMTQ0NzA4ODA4NDU5OA%3D%3D&el=1_x_3&_esc=publicationCoverPdf)

1.  **Organize Data into Frequency Histograms:**

    -   Create frequency histograms for each habitat variable (depth,
        mean column velocity, substrate, distance to bank, cover) using
        the specified bin intervals.

    -   Bin intervals: 0.2 ft for depths and nose heights, 0.2 fps for
        mean column and nose velocities, 0.5 for the substrate code, and
        5.0 ft for distance to bank.

2.  **Develop Habitat Suitability Criteria Curves:**

    -   Fit 4th-order polynomial regression models to the histograms for
        depth, mean column velocity, and substrate.

    -   Use 6 discrete cover codes to develop cover-based HSC.

    -   4th-order polynomials are used for their ability to provide a
        visually realistic fit to skewed HSC data.

3.  **Calculate HSI Values:**

    -   Once the polynomial regression models are fitted, use them to
        calculate HSI values for each habitat variable.

    -   The HSI values should range from 0 to 1, with 1 representing the
        most suitable habitat.

**Notes**

-   binned by 0.1 for depth and 0.01 for velocity

-   TODO mapping for substrate and cover

-   depth and velocity data has outliers removed and is filtered to data
    that has fish present

#### Depth

```{r}
depth_data_all <- mini_snorkel_model_ready |> 
  select(count, depth) |> 
  filter(count > 0) |>
  mutate(weighted_depth = depth * count / sum(count)) |> 
  na.omit()

outlier_threshold <- quantile(depth_data_all$depth, 0.75)

depth_data <- depth_data_all |> 
  filter(depth <= outlier_threshold)

max_depth <- max(depth_data$depth)

depth_bins <- seq(0.1, max_depth + 1, by = 0.1)

depth_hist <- hist(depth_data$depth, breaks = depth_bins, plot = FALSE)

plot(depth_hist)

depth_model <- lm(depth_hist$counts ~ poly(depth_hist$mids, 4)) #mids represent the central value of each bin

depth_predicted <- predict(depth_model, newdata = data.frame(depth = depth_data$depth))

# Calculate HSI values
hsi_depth <- pmax(0, pmin(1, depth_predicted / max(depth_predicted))) # noramlizes predicted values to the bounds of 0 to 1

depth_to_hsi <- data.frame(hsi_depth = hsi_depth, 
           depth = depth_hist$mids)

plot(depth_hist)

# Plotting
ggplot(depth_to_hsi, aes(x = depth, y = hsi_depth)) +
  geom_line() +
  labs(x = "Depth", y = "HSI") +
  ggtitle("Allen (2000): Depth Habitat Suitability Index")

```

#### Velocity

```{r}
velocity_data_all <- mini_snorkel_model_ready |> 
  select(count, velocity) |> 
  filter(count > 0) |>
  na.omit()

outlier_threshold <- quantile(velocity_data_all$velocity, 0.75)

velocity_data <- velocity_data_all |> 
  filter(velocity <= outlier_threshold)

max_velocity <- max(velocity_data$velocity)

velocity_bins <- seq(0, max_velocity+1, by = 0.01)

velocity_hist <- hist(velocity_data$velocity, breaks = velocity_bins, plot = FALSE)

# Fit 4th-order polynomial regression model
velocity_model <- lm(velocity_hist$counts ~ poly(velocity_hist$mids, 4))

vel_predicted <- predict(velocity_model, newdata = data.frame(velocity = velocity_data$velocity))

hsi_vel <- pmax(0, pmin(1, vel_predicted / max(vel_predicted)))
vel_to_hsi <- data.frame(hsi_vel = hsi_vel, 
           mid_vel = velocity_hist$mids)

plot(velocity_hist)

ggplot(vel_to_hsi, aes(x = mid_vel, y = hsi_vel)) +
  geom_line() +
  labs(x = "Velocity", y = "HSI") +
  ggtitle("Allen (2000): Velocity Habitat Suitability Index")


```

#### Substrate

Map the substrate types to unique codes.

| Mini Snorkel Data Type | Substrate Size Range (mm) | Substrate Code |
|------------------------|---------------------------|----------------|
| fine                   | 0-0.05                    | 1              |
| sand                   | 0.05-2                    | 2              |
| small gravel           | 2-50                      | 3              |
| large gravel           | 50-150                    | 4              |
| cobble                 | 150-300                   | 5              |
| boulder                | \>300                     | 6              |

```{r}
cols <- c('percent_fine_substrate', "percent_sand_substrate" , 'percent_small_gravel_substrate', 'percent_large_gravel_substrate', 'percent_cobble_substrate', 'percent_boulder_substrate')


allen_substrate <- mini_snorkel_model_ready |> 
  select(count, fish_presence, cols) |> 
    pivot_longer(cols = cols, names_to = "type", values_to = "percent") |> 
  mutate(substrate_code = case_when(type == "percent_fine_substrate" ~ 1,
                                    type == "percent_sand_substrate" ~ 2,
                                    type == "percent_small_gravel_substrate" ~ 3,
                                    type == "percent_large_gravel_substrate" ~ 4,
                                    type == "percent_cobble_substrate" ~ 5,
                                    type == "percent_boulder_substrate" ~ 6)) |> 
  filter(percent > 20) |> 
  filter(count > 0) |> 
  filter(substrate_code == 3) # |> 
 # mutate(percent = count * (percent / 100)) # normalize to fish count


# outlier_threshold <- quantile(allen_substrate$percent, 0.75)
# 
# depth_data <- depth_data_all |> 
#   filter(depth <= outlier_threshold)

max_percent <- max(allen_substrate$percent)

percent_bins <- seq(0, max_percent, by = 1)

sub_hist <- hist(allen_substrate$percent, breaks = percent_bins, plot = FALSE)

plot(sub_hist)

model <- lm(sub_hist$counts ~ poly(sub_hist$mids, 4)) #mids represent the central value of each bin

sub_predicted <- predict(model, newdata = data.frame(substrate = allen_substrate$percent))

# Calculate HSI values
hsi <- pmax(0, pmin(1, sub_predicted / max(sub_predicted))) # noramlizes predicted values to the bounds of 0 to 1

sub_to_hsi <- data.frame(hsi = hsi, 
           substrate = sub_hist$mids)

# Plotting
ggplot(sub_to_hsi, aes(x = substrate, y = hsi)) +
  geom_line() +
  labs(x = "Substrate %", y = "HSI") +
  ggtitle("Allen (2000): Substrate Habitat Suitability Index")




```

```{r}
knitr::knit_exit()

```

### Mapping of Substrate and Cover

| Mini Snorkel Data Type | Substrate Size Range (mm) | Gard 2023 Code |
|------------------------|---------------------------|----------------|
| fine                   | 0-0.05                    | 0.1            |
| sand                   | 0.05-2                    | 0.1            |
| small gravel           | 2-50                      | 1 - 1.2        |
| large gravel           | 50-150                    | 1.3-4.6        |
| cobble                 | 150-300                   | 6.8 - 8        |
| boulder                | \>300                     | 9              |

This mapping is a work in progress. The codes don't make a lot of sense
and there is uncertainty on how these codes were developed. A next step
would be to discuss the 2023 paper with Gard to get a better
understanding of how the codes were used in his workflow.

```{r eval=FALSE, include=FALSE}
# testing stepwise logistic regression - non-functional

# start with Chinook and substrate 
chn_mini_snorkel <- mini_snorkel_model_ready |> 
  filter(species == "Chinook salmon" | is.na(species)) |> 
  select(fish_presence, percent_fine_substrate:percent_boulder_substrate) 

# Define a recipe
rec <- recipe(fish_presence ~  percent_fine_substrate + percent_sand_substrate + percent_small_gravel_substrate + percent_large_gravel_substrate + percent_boulder_substrate + percent_cobble_substrate, data = chn_mini_snorkel)  |> 
  step_normalize(all_predictors())

# Split the data into training and testing sets
data_split <- initial_split(chn_mini_snorkel, prop = 0.8, strata = "fish_presence")
data_train <- training(data_split)
data_test <- testing(data_split)

# Create a logistic regression model
log_reg <- logistic_reg() |> 
  set_engine("glm") |> 
  set_mode("classification") |> translate()

# Create a workflow
wf <- workflow()  |> 
  add_recipe(rec) |> 
  add_model(log_reg)

stepwise_mod <- wf |> 
  stats::step()
```

```{r eval=FALSE, include=FALSE}
# testing with a fake dataset to figure out NA issues 

set.seed(06221988)

generate_data <- function(n) {
  return(runif(n, min = 0, max = 100))
}

# create fake dataset
data <- data.frame("fish_presence" = rep(c(0,1), 2400),
                   "percent_fine_substrate" = generate_data(100),
                   "percent_sand_substrate" = generate_data(100),
                   "percent_small_gravel_substrate" = generate_data(100),
                   "percent_large_gravel_substrate" = generate_data(100),
                   "percent_cobble_substrate" = generate_data(100), 
                   "percent_boulder_substrate" = generate_data(100)) |> 
  mutate(fish_presence = as.factor(fish_presence))

# Define a recipe
rec <- recipe(fish_presence ~  percent_fine_substrate + percent_sand_substrate + percent_small_gravel_substrate + percent_large_gravel_substrate + percent_boulder_substrate + percent_cobble_substrate, data = data)  

# Split the data into training and testing sets
data_split <- initial_split(data, prop = 0.8, strata = "fish_presence")
data_train <- training(data_split)
data_test <- testing(data_split)

# Create a logistic regression model
log_reg <- logistic_reg() |> 
  set_engine("glm") |> 
  set_mode("classification") |> translate()

# Create a workflow
wf <- workflow()  |> 
  add_recipe(rec) |> 
  add_model(log_reg)

# Train the model
wf_fit <- wf |> 
  fit(data_train)

tidy(wf_fit) 

```
