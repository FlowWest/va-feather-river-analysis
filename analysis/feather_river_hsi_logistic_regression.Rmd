---
title: "Mini Snorkel Feather HSC Using Logistic Regression"
author: "Maddee Rubenson"
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: true
    toc_depth: 3
    number_sections: false
    math_method:
      engine: webtex
      url: https://latex.codecogs.com/svg.image?
  html_document:
    toc: true
    toc_depth: 3
    number_sections: false
    math_method:
      engine: webtex
      url: https://latex.codecogs.com/svg.image?
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)

knitr::opts_chunk$set(echo = TRUE)

theme_set(
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),  
    axis.title.x = element_text(size = 14),  
    axis.title.y = element_text(size = 14)   
  )
)
options(scipen=999)


```

## Objective

Using the mini snorkel data for the Feather River, produce an HSC that
uses cover, substrate, depth and velocity.

```{r}
# read in mini snorkel data
# TODO: this will get updated to read from EDI when the data package is published
micro_hab <- read_csv(here::here('data-raw', 'microhabitat_observations.csv'))
locations <- read_csv(here::here('data-raw', 'survey_locations.csv'))

mini_snorkel_model_ready <- micro_hab |> left_join(locations) |> 
  mutate(count = ifelse(is.na(count), 0, count),
        fish_presence = as.factor(ifelse(count < 1, "0", "1"))) |> 
  glimpse()

```

## Pre-process data

#### Normalize Substrate by Prevalence

This table provides a weighting for each substrate type based on the
overall presence (\>20%) of each substrate type. Use this to normalize
the substrate columns.

```{r}
substrate_percent <- mini_snorkel_model_ready |> 
  group_by(micro_hab_data_tbl_id) |> 
  select(contains('substrate')) |> 
  distinct() |> 
  pivot_longer(cols = c(percent_fine_substrate:percent_boulder_substrate), names_to = "substrate_type", values_to = "percent") |> 
  mutate(substrate_presence_absence = ifelse(percent < 19, 0, 1)) |>  # 20% threshold
  group_by(substrate_type) |> 
  summarise(total_presence = sum(substrate_presence_absence)) |> 
  ungroup() |> 
  mutate(perc_total = total_presence/sum(total_presence))

knitr::kable(substrate_percent |> mutate(perc_total = perc_total*100), digits = 2)

```

Apply substrate normalization values to substrate columns and remove
unnecessary columns

```{r}
mini_snorkel_grouped <- mini_snorkel_model_ready |> 
  # filter(species %in% c('Chinook Salmon', 'Steelhead trout (wild)', 'Steelhead trout, (clipped)') | is.na(species)) |> 
  rowwise() |> 
  mutate(fine_substrate = percent_fine_substrate * (1-substrate_percent[substrate_percent$substrate_type == "percent_fine_substrate", 'perc_total']$perc_total),
         sand_substrate = percent_sand_substrate * (1-substrate_percent[substrate_percent$substrate_type == "percent_sand_substrate", 'perc_total']$perc_total),
         small_gravel = percent_small_gravel_substrate * (1-substrate_percent[substrate_percent$substrate_type == "percent_small_gravel_substrate", 'perc_total']$perc_total),
         large_gravel = percent_large_gravel_substrate * (1-substrate_percent[substrate_percent$substrate_type == "percent_large_gravel_substrate", 'perc_total']$perc_total),
         cobble_substrate = percent_cobble_substrate * (1-substrate_percent[substrate_percent$substrate_type == "percent_cobble_substrate", 'perc_total']$perc_total),
         boulder_substrate = percent_boulder_substrate * (1-substrate_percent[substrate_percent$substrate_type == "percent_boulder_substrate", 'perc_total']$perc_total)) |> 
  select(-c(percent_fine_substrate:percent_boulder_substrate)) |> 
  mutate(woody_debris = sum(percent_large_woody_cover_inchannel, percent_small_woody_cover_inchannel),
         overhead_cover = sum(percent_cover_half_meter_overhead, percent_cover_more_than_half_meter_overhead)
         ) |> 
  select(-c(percent_small_woody_cover_inchannel, percent_large_woody_cover_inchannel, percent_cover_more_than_half_meter_overhead, percent_cover_half_meter_overhead)) |> 
  #filter(species == "Chinook salmon" | is.na(species)) |> 
  select(-count, -channel_geomorphic_unit, -micro_hab_data_tbl_id, -location_table_id, -fish_data_id,
         -focal_velocity, -dist_to_bottom, -fl_mm, -species,  -transect_code, -date, -surface_turbidity) |> 
  select(-(contains("no_cover"))) |> 
  distinct() |> 
  na.omit() |> glimpse()
```

## Logistic Regression: 1 - Using Cover, Substrate, Velocity, and Depth

**Predictors**

-   Depth

-   Velocity

-   Substrate (fine through boulder) normalized by prevalence

-   Woody Debris (`percent_small_woody_cover_inchannel` +
    `percent_large_woody_cover_inchannel`)

-   Overhead Cover (`percent_cover_more_than_half_meter_overhead` +
    `percent_cover_half_meter_overhead`)

-   Submerged Aquatic Vegetation

-   Undercut Bank

**Notes**

-   Chose to remove surface turbidity from predictors because it is not
    a parameter aligned with the strategic plan. This parameter,
    however, was initially found significant in the logistic regression

**Preliminary Results**

-   Significant Predictors include: undercut bank (+), small gravel (-),
    boulder substrate (+), woody debris (+), and overhead cover (+)

#### Build Model

```{r message=FALSE, warning=FALSE}
recipe <- recipe(data = mini_snorkel_grouped, formula = fish_presence ~.) |> 
  # step_poly(all_predictors(), degree = 2) |> x^2
  step_mutate_at(all_numeric_predictors(), fn = asinh) |> # inverse hyperbolic sine as alternative to log that can handle zeros
  step_naomit(all_predictors()) |> # ensure there are no NAs
  step_zv(all_predictors()) |> # ensure there are no columns that are all the same value
  step_normalize(all_numeric_predictors()) # Normalization can facilitate the comparison of the relative importance of different predictors

log_reg <- logistic_reg() |> 
  set_engine("glm") 

log_reg_model <- workflow() |>
  add_recipe(recipe) |>
  add_model(log_reg) |> 
  fit(data = mini_snorkel_grouped)

log_reg_model |> glance()
log_reg_model |> tidy() |> filter(p.value < 0.1)

sig_predictors <- log_reg_model |> tidy() |> filter(p.value < 0.1) |> filter(term != "(Intercept)") |>  pull(term)

predictions <- predict(log_reg_model, mini_snorkel_grouped, type = "prob") |> 
  bind_cols(mini_snorkel_grouped) 

```

```{r eval=FALSE, include=FALSE}
#exploratory to see if we want to change the threshold?
hard_pred_0.5 <- predictions |>
  mutate(.pred = probably::make_two_class_pred(
      estimate = .pred_1,
      levels = levels(fish_presence),
      threshold = .5))  |>
  select(fish_presence, contains(".pred"))

hard_pred_0.5 %>%
  count(.truth = fish_presence, .pred)
```

### Results

The following predictors were found to significantly effect fish
presence: `r paste0(sig_predictors, collapse = ", ")`

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
map(sig_predictors, function(predictor) {
  ggplot(predictions, aes_string(x = predictor, y = ".pred_1")) +
    geom_smooth(method = "glm") +
    geom_point() + 
    labs(title = paste("Predicted Probability vs.", predictor),
         y = 'Probability of Fish Presence')
})


```

```{r echo=FALSE}
# scratch plots
gmean <- function(x) exp(mean(log(x)))

predictions <- predict(log_reg_model, mini_snorkel_grouped, type = "prob") |> 
  bind_cols(mini_snorkel_grouped) |> 
  bind_cols(predict(log_reg_model, mini_snorkel_grouped,type = "conf_int",level = 0.90)) |> 
  select(-fish_presence) |> 
  pivot_longer(cols = c(depth:overhead_cover), names_to = "predictor", values_to = "value") |> 
  group_by(predictor, value) |> 
  summarise(.pred_1 = .pred_1,
            geom_mean_prob = gmean(.pred_1),
            geom_mean_conf_lower_1 = gmean(.pred_lower_1),
            geom_mean_conf_upper_1 = gmean(.pred_upper_1)) |> 
  ungroup()
  
predictions |> 
  filter(predictor == "percent_undercut_bank") |> 
  ggplot(aes(x = value, y = geom_mean_prob)) +
  geom_point(aes(x = value, y = .pred_1), alpha = 0.5, color = "lightgrey") +
  geom_line() +
  geom_ribbon(aes(ymin = geom_mean_conf_lower_1, ymax = geom_mean_conf_upper_1), alpha = 0.3)

predictions |> 
  filter(predictor == "small_gravel") |> 
  ggplot(aes(x = value, y = geom_mean_prob)) +
  geom_point(aes(x = value, y = .pred_1), alpha = 0.5, color = "lightgrey") +
  geom_line() +
  geom_ribbon(aes(ymin = geom_mean_conf_lower_1, ymax = geom_mean_conf_upper_1),  alpha = 0.3)


predictions |> 
  filter(predictor == "boulder_substrate") |> 
  ggplot(aes(x = value, y = geom_mean_prob)) +
  geom_point(aes(x = value, y = .pred_1), alpha = 0.5, color = "lightgrey") +
  geom_line() +
  geom_ribbon(aes(ymin = geom_mean_conf_lower_1, ymax = geom_mean_conf_upper_1), alpha = 0.3)

predictions |> 
  filter(predictor == "woody_debris") |> 
  ggplot(aes(x = value, y = geom_mean_prob)) +
  geom_point(aes(x = value, y = .pred_1), alpha = 0.5, color = "lightgrey") +
  geom_line() +
  geom_ribbon(aes(ymin = geom_mean_conf_lower_1, ymax = geom_mean_conf_upper_1), alpha = 0.3)

predictions |> 
  filter(predictor == "overhead_cover") |> 
  ggplot(aes(x = value, y = geom_mean_prob)) +
  geom_point(aes(x = value, y = .pred_1), alpha = 0.5, color = "lightgrey") +
  geom_line() +
  geom_ribbon(aes(ymin = geom_mean_conf_lower_1, ymax = geom_mean_conf_upper_1), alpha = 0.3) 

```

## Logistic Regression: 2 - Use cover as presence/absence variable

[Beakes et al.
2012](https://www.sfu.ca/biology/faculty/jwmoore/publications/Beakes_etal_RRA_inpress.pdf)
We identified locations with cover when they were within 50 cm of large
woody debris (\>7.5 cm diameter), tall vegetation (\>50 cm above
ground), overhanging vegetation (\<50 cm from waterâ€™s surface), large
boulders (\>17.5 cm diameter), undercut banks, large bedrock crevasses
or combinations of these cover types. Areas without cover included
characteristics such as small vegetation, small substrate (\<17.5 cm
diameter) or filamentous algae.

**Predictors**

-   Cover Presence (1/0): summation of undercut bank, woody debris,
    overhead cover, and submerged aquatic vegetation and greater than
    20%

-   Substrate (fine through boulder) normalized by prevalence

-   Depth

-   Velocity

**Preliminary Results**

-   Cover Presence is a significant predictor and the strongest when
    describing probability of fish presence

-   Boulder (+) and Small Gravel (-) are also significant predictors

```{r}
mini_snork_cover <- mini_snorkel_grouped |> 
  mutate(cover_total = percent_undercut_bank + woody_debris + overhead_cover + percent_submerged_aquatic_veg_inchannel,
         cover_presence = as.factor(ifelse(cover_total >= 20, 1, 0))) |> 
  select(-c(percent_undercut_bank, woody_debris, overhead_cover, percent_submerged_aquatic_veg_inchannel, cover_total)) |> 
  glimpse()
```

```{r}
recipe <- recipe(data = mini_snork_cover, formula = fish_presence ~.) |> 
  # step_poly(all_predictors(), degree = 2) |> x^2
  step_mutate_at(all_numeric_predictors(), fn = asinh) |> # inverse hyperbolic sine as alternative to log that can handle zeros
  step_naomit(all_predictors()) |> # ensure there are no NAs
  step_zv(all_predictors()) |> # ensure there are no columns that are all the same value
  step_normalize(all_numeric_predictors()) # Normalization can facilitate the comparison of the relative importance of different predictors

log_reg <- logistic_reg() |> 
  set_engine("glm") 

log_reg_model <- workflow() |>
  add_recipe(recipe) |>
  add_model(log_reg) |> 
  fit(data = mini_snork_cover)

log_reg_model |> glance()
log_reg_model |> tidy() |> filter(p.value < 0.1)

sig_predictors <- log_reg_model |> tidy() |> filter(p.value < 0.1) |> filter(term != "(Intercept)") |>  pull(term)

predictions <- predict(log_reg_model, mini_snork_cover, type = "prob") |> 
  bind_cols(mini_snork_cover) 

```

#### Results

The following predictors were found to significantly effect fish
presence: `r paste0(sig_predictors, collapse = ", ")`

```{r}
ggplot(predictions, aes(x = cover_presence, y = .pred_1)) +
    geom_boxplot() +
    labs(title = "Predicted Probability vs. cover presence",
         y = 'Probability of Fish Presence')


map(sig_predictors[1:2], function(predictor) {
  ggplot(predictions, aes_string(x = predictor, y = ".pred_1")) +
    geom_smooth(method = "glm") +
    labs(title = paste("Predicted Probability vs.", predictor),
         y = 'Probability of Fish Presence')
})


```

## Logistic Regression: 3 - using dominant cover and substrate

Instead of using percent - treat cover and substrate as
presence/absence. If there percent is \>= 80%, it is treated as present.

**Predictors**

-   fine substrate (1/0)

-   sand substrate (1/0)

-   small gravel (1/0)

-   large gravel (1/0)

-   submerged aquatic veg (1/0)

-   undercut bank (1/0)

-   no cover overhead (1/0)

-   half meter overhead (1/0)

-   cover more than half meter overhead (1/0)

-   velocity

-   depth

**Preliminary Results**

-   Significant Predictors include: fine substrate (+), sand substrate
    (+), small gravel (-), small woody in channel cover (+), submerged
    aquatic veg (-), no overhead cover (-)

**Notes**

-   The highest percent undercut bank in mini snorkel data is 50% and
    therefore this method doesn't find that as a significant predictor
    because all data is considered to not have an undercut bank. Could
    consider adjusting threshold. Could be worth considering using 20%
    as threshold since that is what is used for the Strategic Plan.

```{r}

percent_threshold <- 20

log_3_data <- mini_snorkel_model_ready |> 
  select(fish_presence, depth, velocity, percent_fine_substrate:percent_cover_more_than_half_meter_overhead) |> 
  mutate(fine_substrate = ifelse(percent_fine_substrate >= percent_threshold, 1, 0),
         sand_substrate = ifelse(percent_sand_substrate >= percent_threshold, 1, 0),
         small_gravel_substrate = ifelse(percent_small_gravel_substrate >= percent_threshold, 1, 0),
         large_gravel_substrate = ifelse(percent_large_gravel_substrate >= percent_threshold, 1, 0),
         small_woody_cover_inchannel = ifelse(percent_small_woody_cover_inchannel >= percent_threshold, 1, 0 ),
         large_woody_cover_inchannel = ifelse(percent_large_woody_cover_inchannel >= percent_threshold, 1, 0 ),
         submerged_aquatic_veg_inchannel = ifelse(percent_submerged_aquatic_veg_inchannel >= percent_threshold, 1, 0),
         undercut_bank = ifelse(percent_undercut_bank >= percent_threshold, 1, 0),
         no_cover_ovhead = ifelse(percent_no_cover_overhead >= percent_threshold, 1, 0),
         half_meter_overhead = ifelse(percent_cover_half_meter_overhead >= percent_threshold, 1, 0),
         cover_more_than_half_meter_overhead = ifelse(percent_cover_more_than_half_meter_overhead >= percent_threshold, 1, 0)) |> 
  select(fish_presence, depth, velocity, fine_substrate:cover_more_than_half_meter_overhead) |> 
  glimpse()

```

### Build Model

```{r}

recipe <- recipe(data = log_3_data, formula = fish_presence ~.) |> 
  # step_poly(all_predictors(), degree = 2) |> x^2
  step_mutate_at(all_numeric_predictors(), fn = asinh) |> # inverse hyperbolic sine as alternative to log that can handle zeros
  step_naomit(all_predictors()) |> # ensure there are no NAs
  step_zv(all_predictors()) |> # ensure there are no columns that are all the same value
  step_normalize(all_numeric_predictors()) # Normalization can facilitate the comparison of the relative importance of different predictors

log_reg <- logistic_reg() |> 
  set_engine("glm") 

log_reg_model <- workflow() |>
  add_recipe(recipe) |>
  add_model(log_reg) |> 
  fit(data = log_3_data)

log_reg_model |> glance()
log_reg_model |> tidy() |> filter(p.value < 0.1)

sig_predictors <- log_reg_model |> tidy() |> filter(p.value < 0.1) |> filter(term != "(Intercept)") |>  pull(term)

predictions <- predict(log_reg_model, log_3_data, type = "prob") |> 
  bind_cols(log_3_data) 

```

### Results

The following predictors were found to significantly effect fish
presence: `r paste0(sig_predictors, collapse = ", ")`.

```{r}

predictions |> 
  ggplot(aes(x = as.factor(fine_substrate), y = .pred_1)) +
  geom_boxplot() + 
  labs(title = paste("Predicted Probability vs. fine_substrate"),
         y = 'Probability of Fish Presence')

predictions |> 
  ggplot(aes(x = as.factor(sand_substrate), y = .pred_1)) +
  geom_boxplot() + 
  labs(title = paste("Predicted Probability vs. sand_substrate"),
         y = 'Probability of Fish Presence')

predictions |> 
  ggplot(aes(x = as.factor(small_gravel_substrate), y = .pred_1)) +
  geom_boxplot() + 
  labs(title = paste("Predicted Probability vs. small_gravel_substrate"),
         y = 'Probability of Fish Presence')

predictions |> 
  ggplot(aes(x = as.factor(small_woody_cover_inchannel), y = .pred_1)) +
  geom_boxplot() + 
  labs(title = paste("Predicted Probability vs. small_woody_cover_inchannel"),
         y = 'Probability of Fish Presence')

predictions |> 
  ggplot(aes(x = as.factor(submerged_aquatic_veg_inchannel), y = .pred_1)) +
  geom_boxplot() + 
  labs(title = paste("Predicted Probability vs. submerged_aquatic_veg_inchannel"),
         y = 'Probability of Fish Presence')

predictions |> 
  ggplot(aes(x = as.factor(no_cover_ovhead), y = .pred_1)) +
  geom_boxplot() + 
  labs(title = paste("Predicted Probability vs. no_cover_ovhead"),
         y = 'Probability of Fish Presence')


```

## Logistic Regression 4: Cover Variables by Dominance

Create a hierarchy of cover variables based on their dominance; include
boulder and cobble as cover and remove all other substrate variables
from dataset

```{r}
cover_heirarchy <- mini_snorkel_model_ready |> 
  group_by(micro_hab_data_tbl_id) |> 
  select(count, fish_presence, micro_hab_data_tbl_id, contains("cover"), "percent_cobble_substrate", "percent_boulder_substrate", "percent_undercut_bank") |>
  select(-contains("no_cover")) |> 
  distinct() |> 
  mutate(small_woody_inchannel_and_half_meter_overhead = percent_cover_half_meter_overhead + percent_small_woody_cover_inchannel,
         small_woody_inchannel_and_more_than_half_meter_overhead = percent_cover_more_than_half_meter_overhead + percent_small_woody_cover_inchannel,
         large_woody_inchannel_and_half_meter_overhead = percent_cover_half_meter_overhead + percent_large_woody_cover_inchannel,
         large_woody_inchannel_and_more_than_half_meter_overhead = percent_cover_more_than_half_meter_overhead + percent_large_woody_cover_inchannel) |>
  pivot_longer(cols = c(percent_small_woody_cover_inchannel:large_woody_inchannel_and_more_than_half_meter_overhead), names_to = "habitat", values_to = "percent") |> 
  na.omit() |> 
  mutate(presence_absence = ifelse(percent < 20, 0, 1)) |>   # 20% threshold
  group_by(habitat) |> 
  summarise(total_presence = sum(presence_absence)) |> 
  arrange(desc(total_presence))

cover_heirarchy

percent_threshold <- 20

log_4_data <- mini_snorkel_model_ready |> 
    select(count, fish_presence, micro_hab_data_tbl_id, depth, velocity, contains("inchannel"), contains("overhead"), "percent_cobble_substrate", "percent_boulder_substrate", "percent_undercut_bank") |> 
  # create new cover variables, based off Gard 2023
  mutate(small_woody_inchannel_and_half_meter_overhead = percent_cover_half_meter_overhead + percent_small_woody_cover_inchannel,
         small_woody_inchannel_and_more_than_half_meter_overhead = percent_cover_more_than_half_meter_overhead + percent_small_woody_cover_inchannel,
         large_woody_inchannel_and_half_meter_overhead = percent_cover_half_meter_overhead + percent_large_woody_cover_inchannel,
         large_woody_inchannel_and_more_than_half_meter_overhead = percent_cover_more_than_half_meter_overhead + percent_large_woody_cover_inchannel) |> 
  mutate(cobble_substrate = ifelse(percent_cobble_substrate >= percent_threshold, 1, 0 ),
         boulder_substrate = ifelse(percent_boulder_substrate >= percent_threshold, 1, 0 ),
         small_woody_cover_inchannel = ifelse(percent_small_woody_cover_inchannel >= percent_threshold, 1, 0 ),
         large_woody_cover_inchannel = ifelse(percent_large_woody_cover_inchannel >= percent_threshold, 1, 0 ),
         submerged_aquatic_veg_inchannel = ifelse(percent_submerged_aquatic_veg_inchannel >= percent_threshold, 1, 0),
         undercut_bank = ifelse(percent_undercut_bank >= percent_threshold, 1, 0),
         no_cover_overhead = ifelse(percent_no_cover_overhead >= percent_threshold, 1, 0),
         half_meter_overhead = ifelse(percent_cover_half_meter_overhead >= percent_threshold, 1, 0),
         cover_more_than_half_meter_overhead = ifelse(percent_cover_more_than_half_meter_overhead >= percent_threshold, 1, 0),      
         no_cover_inchannel = ifelse(percent_no_cover_inchannel >= percent_threshold, 1, 0),      
         small_woody_half_meter_overhead = ifelse(small_woody_inchannel_and_half_meter_overhead >= percent_threshold, 1, 0),
         small_woody_cover_more_than_half_meter_overhead = ifelse(small_woody_inchannel_and_more_than_half_meter_overhead >= percent_threshold, 1, 0),
         large_woody_inchannel_and_half_meter_overhead = ifelse(large_woody_inchannel_and_more_than_half_meter_overhead >= percent_threshold, 1, 0),
         large_woody_cover_more_than_half_meter_overhead = ifelse(large_woody_inchannel_and_more_than_half_meter_overhead >= percent_threshold, 1, 0)) |>
  select(fish_presence, depth, velocity, cobble_substrate:large_woody_cover_more_than_half_meter_overhead) |> 
  glimpse()

```

### Build Model

```{r}

recipe <- recipe(data = log_4_data, formula = fish_presence ~.) |> 
  step_zv(all_predictors()) |> # ensure there are no columns that are all the same value
  step_normalize(all_numeric_predictors()) # Normalization can facilitate the comparison of the relative importance of different predictors

log_reg <- logistic_reg() |> 
  set_engine("glm") 

log_reg_model <- workflow() |>
  add_recipe(recipe) |>
  add_model(log_reg) |> 
  fit(data = log_4_data)

log_reg_model |> glance()
log_reg_model |> tidy() |> filter(p.value < 0.1)

sig_predictors <- log_reg_model |> tidy() |> filter(p.value < 0.1) |> filter(term != "(Intercept)") |>  pull(term)

predictions <- predict(log_reg_model, log_4_data, type = "prob") |> 
  bind_cols(log_4_data) 

```

### Results

```{r}
plots <- map(sig_predictors, function(predictor) {
  ggplot(predictions |> na.omit(), aes(x = factor(.data[[predictor]]), y = .pred_1)) +
    geom_boxplot() + 
    labs(title = paste("Predicted Probability vs.", predictor),
         y = 'Probability of Fish Presence',
         x = 'Presence (1) / Absence (0)')
})

plots

```

## Hurdle Model - Binary Cover Values

A hurdle model was used in Gard 2024 to test for the effects of cover
and habitat type on the total abundance of Chinook salmon at both site
and cell level. Here we use the hurdle model to help understand the
influence of velocity, depth, and cover on fish count and
presence/absence.

**Hurdle Models**

Hurdle models are used when count data has an excess of zeros. These
models can be understood as a mixture of two subset of populations. In
one subset, we have a usual count model that may or may not generate
zero, and the other subset only produce zero count.

A hurdle model models excess zeroes separately from the rest of the
data. The zero counts are modeled as a binary response variable and the
positive counts are modeled using poisson distribution.

*Interpreting a Hurdle Model*

The binary part of the model helps identify factors that influence the
presence/absence of fish. The coefficients of the zero part of the
hurdle model represent the odds ratio of observing at least one fish.

The count part of the model estimate the effects of predictor variables
on the count outcome, excluding all zero counts. Coefficients of counts
represent rate ratios of one or more fish observed.

The Incidence Result Ratio (IRR) in the count part of the model (count
\> 0) represent the multiplicative effect of a one-unit change in a
predictor variable on the expected count of non-zero observations,
assuming all other variables are held constant. For example, if the IRR
for a predictor is 1.2, it means that a one-unit increase in that
predictor is associated with a 20% increase in the expected count of
non-zero observations, assuming all other variables remain constant. For
the binary part of the model - if the coefficient for a predictor in the
binary part of the hurdle model is 0.5, it means that a one-unit
increase in the predictor is associated with a 50% increase in the odds
of having a zero count versus a positive count, assuming all other
variables are held constant.

**Notes** For this hurdle model, chose to make all substrate and cover
variables presence/absence based on a 20% threshold

**Predictors**

-   small woody inchannel + half meter overhead
-   small woody inchannel + more than half meter overhead
-   large woody inchannel + half meter overhead
-   large woody inchannel + more than half meter overhead
-   cobble substrate
-   boulder substrate
-   small woody inchannel
-   large woody inchannel
-   submerged aquatic veg inchannel
-   undercut bank
-   more than half meter overhead
-   small woody half meter overhead
-   small woody more than half meter overhead
-   velocity
-   depth

```{r}
# use a hurdle model (Gard 2024); 
# pre-processing - all numeric values must be rounded to whole numbers
hurdle_data <- mini_snorkel_model_ready |>
    select(count, depth, velocity, contains("inchannel"), contains("overhead"), "percent_cobble_substrate", "percent_boulder_substrate", "percent_undercut_bank") |>
  # create new cover variables, based off Gard 2023
  mutate(small_woody_inchannel_and_half_meter_overhead = percent_cover_half_meter_overhead + percent_small_woody_cover_inchannel,
         small_woody_inchannel_and_more_than_half_meter_overhead = percent_cover_more_than_half_meter_overhead + percent_small_woody_cover_inchannel,
         large_woody_inchannel_and_half_meter_overhead = percent_cover_half_meter_overhead + percent_large_woody_cover_inchannel,
         large_woody_inchannel_and_more_than_half_meter_overhead = percent_cover_more_than_half_meter_overhead + percent_large_woody_cover_inchannel) |>
  # transform to presence/absence based on a defined threshold
  mutate(cobble_substrate = ifelse(percent_cobble_substrate >= percent_threshold, 1, 0 ),
         boulder_substrate = ifelse(percent_boulder_substrate >= percent_threshold, 1, 0 ),
         small_woody_cover_inchannel = ifelse(percent_small_woody_cover_inchannel >= percent_threshold, 1, 0 ),
         large_woody_cover_inchannel = ifelse(percent_large_woody_cover_inchannel >= percent_threshold, 1, 0 ),
         submerged_aquatic_veg_inchannel = ifelse(percent_submerged_aquatic_veg_inchannel >= percent_threshold, 1, 0),
         undercut_bank = ifelse(percent_undercut_bank >= percent_threshold, 1, 0),
         no_cover_overhead = ifelse(percent_no_cover_overhead >= percent_threshold, 1, 0),
         half_meter_overhead = ifelse(percent_cover_half_meter_overhead >= percent_threshold, 1, 0),
         cover_more_than_half_meter_overhead = ifelse(percent_cover_more_than_half_meter_overhead >= percent_threshold, 1, 0),
         no_cover_inchannel = ifelse(percent_no_cover_inchannel >= percent_threshold, 1, 0),
         small_woody_half_meter_overhead = ifelse(small_woody_inchannel_and_half_meter_overhead >= percent_threshold, 1, 0),
         small_woody_cover_more_than_half_meter_overhead = ifelse(small_woody_inchannel_and_more_than_half_meter_overhead >= percent_threshold, 1, 0),
         large_woody_inchannel_and_half_meter_overhead = ifelse(large_woody_inchannel_and_more_than_half_meter_overhead >= percent_threshold, 1, 0),
         large_woody_cover_more_than_half_meter_overhead = ifelse(large_woody_inchannel_and_more_than_half_meter_overhead >= percent_threshold, 1, 0)) |>
  select(count, depth, velocity, cobble_substrate:large_woody_cover_more_than_half_meter_overhead) |>
  # remove no cover variables (no cover overhead, no cover in channel)
  select(-contains("no_cover")) |>
  mutate_all(~ round(., digits = 0)) |>
  na.omit() |>
  glimpse()


hurdle_model <- pscl::hurdle(count ~ small_woody_cover_inchannel + depth + velocity + large_woody_cover_inchannel + submerged_aquatic_veg_inchannel + cover_more_than_half_meter_overhead + half_meter_overhead + cobble_substrate + boulder_substrate + undercut_bank, data = hurdle_data, dist = "negbin") 
# Negative Binomial Distribution: The Negative Binomial distribution is more flexible than the Poisson distribution and is used when the variance of the counts is greater than the mean, which is known as overdispersion. The Negative Binomial model allows for the variance to be larger than the mean, which is common in count data where there is extra variability beyond what is expected from a Poisson distribution.


# zero inflated models are capable of dealing with excess zero counts
# NOTE: Chose to not use a zero inflated model because it operates under the assumption that excess zeros are generated by a separate process from the count values. 
# zeroinf_model <- pscl::zeroinfl(count ~ ., data = hurdle_data) # , zero.dist = "binomial", dist = "negbin"

best_model_hurdle <- MASS::stepAIC(hurdle_model, trace = TRUE)

summary(best_model_hurdle)

```

### Results

```{r echo=FALSE}
library(sjPlot)
library(emmeans)

tab_model(best_model_hurdle)

# emmip(best_model_hurdle, small_woody_half_meter_overhead ~ boulder_substrate + large_woody_cover_inchannel + undercut_bank + cover_more_than_half_meter_overhead,
#       at = list(
#         small_woody_half_meter_overhead = 0:1, 
#         boulder_substrate = 1, 
#         large_woody_cover_inchannel = 1, 
#         undercut_bank = 1,
#         cover_more_than_half_meter_overhead = 1, 
#         no_cover_inchannel = 1
#       ),
#        lin.pred = FALSE, mode ="prob0", CIs = TRUE) +
#   ylab("Probability of zero")
# 
# emmip(best_model_hurdle, undercut_bank ~  boulder_substrate + large_woody_cover_inchannel + small_woody_half_meter_overhead + cover_more_than_half_meter_overhead,
#        lin.pred = FALSE, mode ="prob0", CIs = TRUE) +
#   ylab("Probability of zero")

# emmip(best_model_hurdle, percent_undercut_bank ~  percent_boulder_substrate,
#        lin.pred = FALSE, mode ="prob0", CIs = TRUE) +
#   ylab("Probability of zero")

```

## Hurdle Model - with values

This hurdle model uses substrate and cover percent values.

**Predictors**

-   depth

-   velocity

-   percent large woody cover in channel

-   percent small woody cover in channel

-   percent cover half meter overhead

-   percent cover more than half meter overhead

-   percent cobble

-   percent boulder

-   percent undercut bank

**Notes** \*

-   removed combined cover variables (small and large woody in channel
    with cover overhead) because of issues with collinearity within the
    hurdle model

```{r}

# use a hurdle model (Gard 2024); 
# pre-processing - all numeric values must be rounded to whole numbers

# WIP
hurdle_data <- mini_snorkel_model_ready |>
    select(count, depth, velocity, contains("inchannel"), contains("overhead"), "percent_cobble_substrate", "percent_boulder_substrate", "percent_undercut_bank") |>
  # create new cover variables, based off Gard 2023
  mutate(small_woody_inchannel_and_half_meter_overhead = percent_cover_half_meter_overhead + percent_small_woody_cover_inchannel,
         small_woody_inchannel_and_more_than_half_meter_overhead = percent_cover_more_than_half_meter_overhead + percent_small_woody_cover_inchannel,
         large_woody_inchannel_and_half_meter_overhead = percent_cover_half_meter_overhead + percent_large_woody_cover_inchannel,
         large_woody_inchannel_and_more_than_half_meter_overhead = percent_cover_more_than_half_meter_overhead + percent_large_woody_cover_inchannel) |>
  # remove no cover variables (no cover overhead, no cover in channel)
  select(-contains("no_cover")) |>
  #mutate_all(~ round(., digits = 0)) |>
  na.omit() |>
  distinct() |>
  glimpse()

hurdle_model <- pscl::hurdle(count ~ percent_small_woody_cover_inchannel + depth + velocity + percent_large_woody_cover_inchannel + percent_submerged_aquatic_veg_inchannel + percent_cover_more_than_half_meter_overhead + percent_cover_half_meter_overhead + percent_cobble_substrate + percent_boulder_substrate + percent_undercut_bank, data = hurdle_data, dist = "negbin") 
# Negative Binomial Distribution: The Negative Binomial distribution is more flexible than the Poisson distribution and is used when the variance of the counts is greater than the mean, which is known as overdispersion. The Negative Binomial model allows for the variance to be larger than the mean, which is common in count data where there is extra variability beyond what is expected from a Poisson distribution.

best_model_hurdle <- MASS::stepAIC(hurdle_model, trace = TRUE)

summary(best_model_hurdle)

```

### Results

```{r}
tab_model(best_model_hurdle)

```

```{r}
knitr::knit_exit()
```

## Next steps

1.  Compare to species presence/absence
2.  Look into methods for creating a more even sample of fish
    presence/absence that does not create a synthetic dataset
3.  Continue exploring methods for normalization and tuning parameters
    and adding interactions between parameters
4.  Explore using a fixed effect - such as geomorphic unit (pool, glide,
    riffle) or river mile

```{r}
# explore interactions

recipe <- recipe(data = mini_snorkel_grouped, formula = fish_presence ~.) |> 
  step_mutate_at(all_numeric_predictors(), fn = asinh) |> # inverse hyperbolic sine as alternative to log that can handle zeros
  step_naomit(all_predictors()) |> # ensure there are no NAs
  step_zv(all_predictors()) |> # ensure there are no columns that are all the same value
  step_normalize(all_numeric_predictors()) # Normalization can facilitate the comparison of the relative importance of different predictors

log_reg <- logistic_reg() |> 
  set_engine("glm") 

log_reg_model <- workflow() |>
  add_recipe(recipe) |>
  add_model(log_reg) |> 
  fit(data = mini_snorkel_grouped)

interaction_grid <- mini_snorkel_grouped |> 
  select(-fish_presence) |> 
  names() |> 
  combn(2, simplify = FALSE) |> 
  map(~interaction(term = .x[[1]], .x[[2]])) |> 
  unlist()

folds <- vfold_cv(mini_snorkel_grouped, v = 5)
model_with_interactions <- tune_grid(
  log_reg_model,
   resamples = folds,
  grid = interaction_grid
)

show_best(model_with_interactions) 

```

```{r}
# using a single predictor: 
# RESULTS ARE ALIGNED 
recipe <- recipe(data = mini_snorkel_grouped, formula = fish_presence ~ percent_undercut_bank) |> 
  step_mutate_at(all_numeric_predictors(), fn = asinh) |> # inverse hyperbolic sine as alternative to log that can handle zeros
  step_naomit(all_predictors()) |> # ensure there are no NAs
  step_zv(all_predictors()) |> # ensure there are no columns that are all the same value
  step_normalize(all_numeric_predictors()) # Normalization can facilitate the comparison of the relative importance of different predictors

log_reg <- logistic_reg() |> 
  set_engine("glm") 

log_reg_model <- workflow() |>
  add_recipe(recipe) |>
  add_model(log_reg) |> 
  fit(data = mini_snorkel_grouped)

log_reg_model |> tidy()

predictions <- predict(log_reg_model, mini_snorkel_grouped, type = "prob") |> 
  bind_cols(mini_snorkel_grouped) 

ggplot(predictions, aes(x = percent_undercut_bank, y = .pred_1)) +
    geom_smooth(method = "glm") 

```
