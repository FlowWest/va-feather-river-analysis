---
title: "Feather River - occupancy model"
author: "Ashley Vizek (FlowWest)"
date: "`r Sys.Date()`"
output: 
  html_document
---

# Overview

This markdown builds off the `occupancy_model.Rmd` and tries a different grouping approach. The original uses `locations_revised` to structure the data. Here we will try a few other options:

- use the results of the cluster analysis to group observations

Other ideas include:

- use another covariate such as temperature (though we would need to get that data)
- create groups based on velocity or depth


```{r, include = F, message = F, warning = F}
library(tidyverse)
library(googleCloudStorageR)
knitr::opts_chunk$set(warning = F, message = F)

ongoing_location_lookup_raw <- read_csv(here::here("data", "feather_location_lookup.csv"))
ongoing_metadata_raw <- read_csv(here::here("data", "feather_snorkel_metadata.csv"))
ongoing_obs_raw <- read_csv(here::here("data", "feather_snorkel_observations.csv"))

# mini snorkel data
# TODO is there a table that associates the microhabitat plot with a lat/long?
# TODO the survey locations are missing coordinates
mini_snorkel_raw <- read_csv(here::here("data", "microhabitat_with_fish_observations.csv"))
location_raw <- read_csv(here::here("data", "survey_locations.csv"))
```

```{r, include = F}
mini_snorkel_filtered <- mini_snorkel_raw |> 
  # associate location table so we can group by the reach if needed.
  # note that the location_revised field is still undergoing review with Ryon
  left_join(location_raw |> 
              select(location_table_id, location_revised)) |> 
  # Ryon thought 140 was better than 100. TODO look into this more. goal to filter to YOY
  filter(fl_mm < 140 | is.na(fl_mm)) |> 
  mutate(month = month(date))

mini_snorkel_filtered |> 
  group_by(month = month(date)) |> 
  summarize(count = sum(count, na.rm = T)) |> 
  ggplot(aes(x = month, y = count)) +
  geom_col()

mini_snorkel_filtered |> 
  mutate(month = month(date)) |> 
  select(location_revised, month) |> 
  distinct() |> 
  group_by(location_revised) |> 
  tally() |> 
  ggplot(aes(x = location_revised, y = n)) +
           geom_col()

mini_snorkel_filtered_march <- mini_snorkel_filtered |> 
  filter(month(date) == 3) 

# summarize the number of sites sampled - looks like there is one steep riffle for 3/13 and one for 3/15. this may be due to incorrect location cateogorization. for now lets call all location_revised as one single site

# summarize the number of replicates per site - as expected 35-36 for each

# decide on how to categorize cover
# woody debris, submerged aquatic, undercut, overhead - lets try this for now (select dominant)

mini_snorkel_filtered_march |> 
  distinct(location_revised, date) |> 
  arrange(location_revised)

mini_snorkel_filtered_march |> 
  distinct(location_revised, transect_code) |> 
  group_by(location_revised) |> 
  tally()
```

```{r, include = F}

# processing cover by: 1. joining large/small woody  2. joining the two overhead levels 3. assigning highest percent 4. assigned 0 if NA

mini_snorkel_filtered_cover <- mini_snorkel_filtered_march |> 
  select(location_revised, transect_code, percent_small_woody_cover_inchannel, percent_large_woody_cover_inchannel, percent_submerged_aquatic_veg_inchannel, percent_undercut_bank, percent_cover_half_meter_overhead, percent_cover_more_than_half_meter_overhead) |> 
  distinct() |> 
  mutate(woody = percent_small_woody_cover_inchannel + percent_large_woody_cover_inchannel,
         overhead = percent_cover_half_meter_overhead + percent_cover_more_than_half_meter_overhead) |> 
  select(-c(percent_small_woody_cover_inchannel, percent_large_woody_cover_inchannel, percent_cover_half_meter_overhead, percent_cover_more_than_half_meter_overhead)) |> 
  rename(aquatic = percent_submerged_aquatic_veg_inchannel,
         undercut = percent_undercut_bank) |> 
  pivot_longer(cols = c(woody, aquatic, undercut, overhead)) |> 
  group_by(location_revised, transect_code) |> 
  slice_max(value, with_ties = F) |> 
  mutate(cover_type = ifelse(value == 0, "no cover", name)) |> 
  select(-c(name, value))

full_mini_snorkel_data <- mini_snorkel_filtered |> 
  left_join(mini_snorkel_filtered_cover)

# This can be used to clean the data if needed. If there are ties they happen when there are 0s
# so for now adjusting to remove ties from slice_max. Remove this when ready to clean
mini_snorkel_filtered_march_cover |> 
  group_by(micro_hab_data_tbl_id, location_table_id, transect_code, date) |> 
  summarize(sum = sum(value),
            n = length(transect_code)) |> 
  filter(n > 1)

```

```{r, include = F}
library(unmarked)
# sets up mini snorkel data for occupancy model
# habitat variables of interest based on literature
# depth, velocity, substrate, cover

# site level habitat variables
# we would need to take an average for the entire site based on micro level information
# TODO are there better site level variables to use?
# channel geomorphic is collected at microhab level it appears
# use average depth and velocity for now
# including cover by taking average percent of types and then applying same dominant methodology. could do this in a different way

average_site_cover <- mini_snorkel_filtered |> 
  select(location_revised, transect_code, percent_small_woody_cover_inchannel, percent_large_woody_cover_inchannel, percent_submerged_aquatic_veg_inchannel, percent_undercut_bank, percent_cover_half_meter_overhead, percent_cover_more_than_half_meter_overhead) |> 
  distinct() |> 
  group_by(location_revised) |> 
  summarise(percent_small_woody_cover_inchannel = mean(percent_small_woody_cover_inchannel),
            percent_large_woody_cover_inchannel = mean(percent_large_woody_cover_inchannel),
            percent_cover_half_meter_overhead = mean(percent_cover_half_meter_overhead),
            percent_cover_more_than_half_meter_overhead = mean(percent_cover_more_than_half_meter_overhead),
            percent_submerged_aquatic_veg_inchannel = mean(percent_submerged_aquatic_veg_inchannel),
            percent_undercut_bank = mean(percent_undercut_bank)) |> 
  mutate(woody = percent_small_woody_cover_inchannel + percent_large_woody_cover_inchannel,
         overhead = percent_cover_half_meter_overhead + percent_cover_more_than_half_meter_overhead) |> 
  select(-c(percent_small_woody_cover_inchannel, percent_large_woody_cover_inchannel, percent_cover_half_meter_overhead, percent_cover_more_than_half_meter_overhead)) |> 
  rename(aquatic = percent_submerged_aquatic_veg_inchannel,
         undercut = percent_undercut_bank) |> 
  pivot_longer(cols = c(woody, aquatic, undercut, overhead)) |> 
  group_by(location_revised) |> 
  slice_max(value, with_ties = F) |> 
  rename(percent_cover = value) |> 
  select(-c(name))
 
average_site_level_covariates <- full_mini_snorkel_data |> 
  select(-c(fish_data_id, species, date, channel_geomorphic_unit, fl_mm, count, dist_to_bottom, focal_velocity)) |> 
  distinct() |> # make sure we just have one row for each quadrat (if multiple types of fish/fl observed would be repeat rows)
    mutate(
         mean_velocity = mean(velocity), # scale variables
         mean_depth = mean(depth),
         sd_velocity = sd(velocity),
         sd_depth = sd(depth),
         velocity = ((velocity - mean_velocity)/sd_velocity),
         depth = ((depth - mean_depth)/sd_depth)) |> 
  pivot_longer(cols = c(depth:velocity), names_to = "parameter", values_to = "value") |> 
  group_by(location_revised, parameter) |> 
  summarize(value = mean(value, na.rm = T)) |>  # decided to do mean because there are a lot of zeros; we could also choose min or max
  pivot_wider(id_cols = location_revised, values_from = "value", names_from = "parameter") |> 
  left_join(average_site_cover)

# quadrat level fish observations variables
quadrat_fish_obs <- full_mini_snorkel_data |> 
  # decided not to do species filtering for now
  # filter(species == "Chinook salmon" | grepl("Steelhead", species) | is.na(species)) |> 
  select(location_revised, transect_code, count) |> 
  group_by(location_revised, transect_code) |> 
  summarize(count = sum(count, na.rm = T)) |> 
  mutate(count = ifelse((is.na(count) | count == 0), 0, 1)) |> 
  group_by(location_revised) |> 
  mutate(label = row_number()) 

quadrat_fish_obs_wide <- quadrat_fish_obs |> 
  pivot_wider(id_cols = location_revised, names_from = "label", values_from = "count")

# preparing covariates for observations. focal velocity and depth will vary by species obs. we can take the average or we can use the depth and velocity associated with the quadrat
# decided just to use cover for now
quadrat_cover <- quadrat_fish_obs |> 
              select(-count) |> 
  left_join(mini_snorkel_filtered_march_cover) |> 
  mutate(cover_type = factor(cover_type))

cover <- quadrat_cover |> 
  pivot_wider(id_cols = location_revised, names_from = "label", values_from = "cover_type")

quadrat_turbidity <- quadrat_fish_obs |> 
  select(-count) |> 
  left_join(full_mini_snorkel_data |> 
              group_by(location_revised, transect_code) |> 
              summarize(turbidity = mean(surface_turbidity, na.rm = T))) |> 
  mutate(mean_turbidity = mean(turbidity),
         sd_turbidity = sd(turbidity),
         turbidity = ifelse(mean_turbidity==0 & sd_turbidity==0, 0, ((turbidity - mean_turbidity)/sd_turbidity))) |> 
  select(-c(mean_turbidity, sd_turbidity))

turbidity <- quadrat_turbidity |> 
  pivot_wider(id_cols = location_revised, names_from = "label", values_from = "turbidity")


# decided to only use cover for now
# depth <- quadrat_hab_vars  |> 
#   select(location_table_id, label, depth) |> 
#   ungroup() |> 
#   mutate(label = paste0("depth.", label),
#          mean= mean(depth),
#          sd = sd(depth),
#          depth = ((depth - mean)/sd)) |> 
#   pivot_wider(id_cols = location_table_id, names_from = "label", values_from = "depth")

# velocity
# velocity <- quadrat_hab_vars  |> 
#   select(location_table_id, label, velocity) |> 
#   ungroup() |> 
#   mutate(label = paste0("velocity.", label),
#          mean = mean(velocity, na.rm = T),
#          sd = sd(velocity, na.rm = T),
#          velocity = ((velocity - mean)/sd)) |> 
#   pivot_wider(id_cols = location_table_id, names_from = "label", values_from = "velocity")

# # large sub
# large_sub <- quadrat_hab_vars  |> 
#   select(location_table_id, label, percent_large_substrate) |> 
#   ungroup() |> 
#   mutate(label = paste0("substrate.", label),
#          mean = mean(percent_large_substrate),
#          sd = sd(percent_large_substrate),
#          percent_large_substrate = ((percent_large_substrate - mean)/sd)) |> 
#   pivot_wider(id_cols = location_table_id, names_from = "label", values_from = "percent_large_substrate")

# instream
# instream <- quadrat_hab_vars  |> 
#   select(location_table_id, label, percent_instream_cover) |> 
#   ungroup() |> 
#   mutate(label = paste0("instreamcover.", label),
#          mean = mean(percent_instream_cover),
#          sd = sd(percent_instream_cover),
#          percent_instream_cover = ((percent_instream_cover - mean)/sd)) |> 
#   pivot_wider(id_cols = location_table_id, names_from = "label", values_from = "percent_instream_cover")

# overhead
# overhead <- quadrat_hab_vars  |> 
#   select(location_table_id, label, percent_overhead_cover) |> 
#   ungroup() |> 
#   mutate(label = paste0("overheadcover.", label),
#          mean = mean(percent_overhead_cover),
#          sd = sd(percent_overhead_cover),
#          percent_overhead_cover = ((percent_overhead_cover - mean)/sd)) |> 
#   pivot_wider(id_cols = location_table_id, names_from = "label", values_from = "percent_overhead_cover")

y <- quadrat_fish_obs_wide[,2:37]
siteCovs <- average_site_level_covariates[, c("depth", "velocity", "percent_cover")]
obsCovs <- list(cover = cover[,2:37],
                turbidity = turbidity[,2:37])

dat <- unmarkedFrameOccu(y = y, siteCovs = siteCovs, obsCovs = obsCovs)
summary(dat)
plot(dat)
# example datasets within unmarked package
# https://cran.r-project.org/web/packages/unmarked/vignettes/unmarked.html

# wt <- read.csv(system.file("csv","widewt.csv", package="unmarked"))
# y <- wt[,2:4]
# siteCovs <-  wt[,c("elev", "forest", "length")]
# obsCovs <- list(date=wt[,c("date.1", "date.2", "date.3")],
#     ivel=wt[,c("ivel.1",  "ivel.2", "ivel.3")])
# wt <- unmarkedFrameOccu(y = y, siteCovs = siteCovs, obsCovs = obsCovs)
# summary(wt)
# wt <- read_csv(system.file("csv", "frog2001pcru.csv", package = "unmarked"))
# pcru <- csvToUMF(system.file("csv","frog2001pcru.csv", package="unmarked"),
#                  long = TRUE, type = "unmarkedFrameOccu")
```

```{r, eval = F, include = F}
# TODO need to better understand detection v occupancy
# Considerations - should this be an open model because fish could be migrating downstream

# ~ detection ~ occupancy

# Null model
fm1 <- occu(~1 ~1, dat)
fm1
# Occupancy probability is 0.99 (nearly 1) and detection probability is 0.17
backTransform(fm1, type = "state")
backTransform(fm1, type = "det")

# Detection is modeled with depth and velocity
fm2 <- occu(~depth+velocity ~1, dat)
fm2


# The expected probability that a site was occupied is 0.99. This estimate applies to the hypothetical population of all possible sites, not the sites found in our sample. For a good discussion of population-level vs finite-sample inference, see Royle and Dorazio (2008) page 117. Note also that finite-sample quantities can be computed in unmarked using empirical Bayes methods as demonstrated at the end of this document.
backTransform(fm2, type = "state")

# Thus, we can say that the expected probability of detection was 0.17 when depth and velocity are fixed at their mean value. 
backTransform(linearComb(fm2, coefficients = c(1,0,0), type = 'det'))
# predictions increase as velocity increases but never above 0.3
newData <- data.frame(depth = 0, velocity = -2:2)
round(predict(fm2, type = 'det', newdata = newData, appendData=TRUE), 2)
# predictions decrease as depth decreases but never above 0.28
newData <- data.frame(depth = -2:2, velocity = 0)
round(predict(fm2, type = 'det', newdata = newData, appendData=TRUE), 2)
# newData <- data.frame(depth = -2:2, velocity = 0, instream_cover = 0)
# newData <- data.frame(depth = 0, velocity = 0, instream_cover = -2:2)

# Detection is modeled with cover and turbidty
# highest prob of detection is with overhead and low turbidity
# high turbidity decreases prob of detection
fm3 <- occu(~cover+turbidity ~1, dat)
fm3 # overhead is the only significant variable
newData <- data.frame(cover = c("aquatic","no cover", "overhead", "woody", "undercut"),
                      turbidity = 0)
newData <- data.frame(cover = c("aquatic","no cover", "overhead", "woody", "undercut"),
                      turbidity = -2)
newData <- data.frame(cover = c("aquatic","no cover", "overhead", "woody", "undercut"),
                      turbidity = 2)
predict(fm3, type = 'det', newdata = newData, appendData=TRUE)

# Detection is modeled with turbidity
# If turbidity held at mean detection probability is 0.17, if decreased then 0.20, if increased then 0.14

fm_turbid <- occu(~turbidity ~1, dat)
fm_turbid
backTransform(linearComb(fm_turbid, coefficients = c(1,-2), type = 'det')) # this line of code allows us to see how detection probability changes when adjust turbidity

fm_cover <- occu(~cover ~1, dat)
fm_cover
newData <- data.frame(cover = c("aquatic","no cover", "overhead", "woody", "undercut"))
predict(fm_cover, type = 'det', newdata = newData, appendData=TRUE)

# Detection is modeled with cover and occupancy is modeled with depth and velocity
fm4 <- occu(~cover ~depth+velocity, dat)
fm4
# how do predictions change for different cover types
newData <- data.frame(cover = c("aquatic","no cover", "overhead", "woody", "undercut"))
predict(fm4, type = 'det', newdata = newData, appendData=TRUE)
# how do prediction of occupancy change based on depth/velocity
newData <- data.frame(depth = -2:2, velocity = 0)
newData <- data.frame(depth = 0, velocity = -2:2)
predict(fm4, type = 'state', newdata = newData, appendData=TRUE)

# Detection modeled with cover and turbidity, occupancy modeled by depth, velocity and cover
full_mod <- occu(~cover+turbidity ~depth+velocity+percent_cover, dat)
full_mod
```
